{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 1 \n",
    "#### Input e download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus.reader import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('europarl_raw')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the raw of EuroparlCorpusReader of different language\n",
    "save **800000** words in **english** and **200000** words **for each other language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252605"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = LazyCorpusLoader('europarl_raw/english', EuroparlCorpusReader, r'ep-.*\\.en', encoding='utf-8')\n",
    "italian =  LazyCorpusLoader('europarl_raw/italian', EuroparlCorpusReader, r'ep-.*\\.it', encoding='utf-8')\n",
    "french =   LazyCorpusLoader('europarl_raw/french', EuroparlCorpusReader, r'ep-.*\\.fr', encoding='utf-8')\n",
    "spanish = LazyCorpusLoader('europarl_raw/spanish', EuroparlCorpusReader, r'ep-.*\\.es', encoding='utf-8')\n",
    "german = LazyCorpusLoader('europarl_raw/german', EuroparlCorpusReader, r'ep-.*\\.de', encoding='utf-8')\n",
    "\n",
    "words = english.raw()[:800000]\n",
    "words += italian.raw()[:200000]\n",
    "words += french.raw()[:200000]\n",
    "words += spanish.raw()[:200000]\n",
    "words += german.raw()[:200000]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words=tokenizer.tokenize(words)\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of stopword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "stop_words_list += stopwords.words('italian')\n",
    "stop_words_list += stopwords.words('french')\n",
    "stop_words_list += stopwords.words('spanish')\n",
    "stop_words_list += stopwords.words('german')\n",
    "stop_words_list = set(stop_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Reduce the BoW removing the **stopword** and **use lemmatizer and stemming** for reduce the words to the **roots** after this make a shuffle of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer= PorterStemmer()\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    s = stemmer.stem(w)\n",
    "    w = lemmatizer.lemmatize(s)\n",
    "    if w not in stop_words_list :\n",
    "        filtered_sentence.append(w)\n",
    "np.random.shuffle(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Calculate the **frequency distribution** of the words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133091"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_distribution = nltk.FreqDist([word.lower() for word in filtered_sentence])\n",
    "\n",
    "len(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only the first **8000 words most used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = [i[0] for i in word_frequency_distribution.most_common(8000)]\n",
    "word_features2 = list(word_frequency_distribution.keys())[:8000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create **featuresets** taken **8000 sentences for english** and **2000 for each of other language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [ (find_features(sentence),\"eng\" ) for sentence in english.sents()[:8000]]\n",
    "featuresets += [ (find_features(sentence),\"not eng\" ) for sentence in italian.sents()[:2000]]\n",
    "featuresets += [ (find_features(sentence),\"not eng\" ) for sentence in french.sents()[:2000]]\n",
    "featuresets += [ (find_features(sentence),\"not eng\" ) for sentence in spanish.sents()[:2000]]\n",
    "featuresets += [ (find_features(sentence),\"not eng\" ) for sentence in german.sents()[:2000]]\n",
    "np.random.shuffle(featuresets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split featuresets in **80:20** and obtain **Training and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = featuresets[0:int(len(featuresets) * 0.8)]\n",
    "testing = featuresets[int(len(featuresets) * 0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train my classifier with training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all the metric\n",
    "#### Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 96.4375\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not eng Precision: 0.9986541049798116\n",
      "not eng Recall: 0.9298245614035088\n",
      "not eng f1_measure: 0.963011031797534\n",
      "\n",
      "eng Precision: 0.9346557759626605\n",
      "eng Recall: 0.9987531172069826\n",
      "eng f1_measure: 0.9656419529837251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, gold_labels = defaultdict(set), defaultdict(set)\n",
    "\n",
    "for i, (features, label) in enumerate(testing):\n",
    "    predictions[classifier.classify(features)].add(i)\n",
    "    gold_labels[label].add(i) \n",
    "\n",
    "for label in predictions:\n",
    "    print(label, 'Precision:', precision(gold_labels[label], predictions[label]))\n",
    "    print(label, 'Recall:', recall(gold_labels[label], predictions[label]))\n",
    "    print(label, 'f1_measure:', f_measure(gold_labels[label], predictions[label]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        |         n |\n",
      "        |         o |\n",
      "        |         t |\n",
      "        |           |\n",
      "        |    e    e |\n",
      "        |    n    n |\n",
      "        |    g    g |\n",
      "--------+-----------+\n",
      "    eng |<1602>   2 |\n",
      "not eng |  112<1484>|\n",
      "--------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_result = []\n",
    "gold_result = []\n",
    "\n",
    "for i in range(len(testing)):\n",
    "    test_result.append(classifier.classify(testing[i][0]))\n",
    "    gold_result.append(testing[i][1])\n",
    "CM = nltk.ConfusionMatrix(gold_result, test_result)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show most informative features of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  report = True              eng : not en =    203.9 : 1.0\n",
      "                    time = True              eng : not en =    141.8 : 1.0\n",
      "                    area = True              eng : not en =     81.8 : 1.0\n",
      "                  regard = True              eng : not en =     69.1 : 1.0\n",
      "                    dire = True           not en : eng    =     45.6 : 1.0\n",
      "                    être = True           not en : eng    =     44.1 : 1.0\n",
      "                     law = True              eng : not en =     37.0 : 1.0\n",
      "              commission = True           not en : eng    =     31.0 : 1.0\n",
      "                  common = True              eng : not en =     30.2 : 1.0\n",
      "                   prima = True           not en : eng    =     27.6 : 1.0\n",
      "                 mention = True              eng : not en =     27.0 : 1.0\n",
      "                 opinion = True              eng : not en =     26.2 : 1.0\n",
      "                  market = True              eng : not en =     24.1 : 1.0\n",
      "                    task = True              eng : not en =     19.7 : 1.0\n",
      "                     fin = True           not en : eng    =     19.0 : 1.0\n",
      "                    risk = True              eng : not en =     18.2 : 1.0\n",
      "                    long = True              eng : not en =     17.0 : 1.0\n",
      "                 natural = True              eng : not en =     17.0 : 1.0\n",
      "                national = True              eng : not en =     16.6 : 1.0\n",
      "                   civil = True              eng : not en =     16.4 : 1.0\n",
      "                   close = True              eng : not en =     16.4 : 1.0\n",
      "                   crime = True              eng : not en =     15.7 : 1.0\n",
      "                    home = True              eng : not en =     15.7 : 1.0\n",
      "                   total = True              eng : not en =     13.0 : 1.0\n",
      "                    sort = True              eng : not en =     12.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(text):\n",
    "    text= word_tokenize(text)\n",
    "    feat = find_features(text)\n",
    "    return classifier.classify(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Classifier passing String in different language\n",
    "##### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test(\"The test of the classifier shall give out the measures of accuracy, precision, recall on the obtained confusion matrix and WILL NOT BE EVALUATED ON THE LEVEL OF THE PERFORMANCES.\\\n",
    " In other terms, when the confusion is produced, then the value of the assignment will be good, independently of the percentage of false positive and negative results.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not eng\n"
     ]
    }
   ],
   "source": [
    "print(test(\"Il test del classificatore darà le misure di accuratezza, precisione, richiamo sulla matrice di confusione ottenuta e NON SARÀ VALUTATO SUL LIVELLO DELLE PRESTAZIONI.\\\n",
    "     In altri termini, quando si produrrà la confusione, allora il valore dell'incarico sarà buono, indipendentemente dalla percentuale di risultati falsi positivi e negativi.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not eng\n"
     ]
    }
   ],
   "source": [
    "print(test(\"Le test du classificateur doit donner les mesures d'exactitude, de précision, de rappel sur la matrice de confusion obtenue et NE SERA PAS ÉVALUÉ AU NIVEAU DES PERFORMANCES.\\\n",
    "     En d'autres termes, lorsque la confusion est produite, alors la valeur de l'affectation sera bonne, indépendamment du pourcentage de résultats faux positifs et négatifs.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not eng\n"
     ]
    }
   ],
   "source": [
    "print(test(\"La prueba del clasificador deberá dar las medidas de exactitud, precisión, recordación sobre la matriz de confusión obtenida y NO SERÁ EVALUADA EN EL NIVEL DE LAS ACTUACIONES.\\\n",
    "     En otros términos, cuando se produzca la confusión, entonces el valor de la asignación será bueno, independientemente del porcentaje de resultados falsos positivos y negativos.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not eng\n"
     ]
    }
   ],
   "source": [
    "print(test(\"Der Test des Klassifikators gibt die Maße für Genauigkeit, Präzision und Wiedererinnerung an der erhaltenen Konfusionsmatrix an und WIRD NICHT AUF DER NIVEAU DER LEISTUNGEN \\\n",
    "    BEWERTET. Mit anderen Worten, wenn die Verwirrung zustande kommt, dann ist der Wert der Zuordnung gut, unabhängig vom Prozentsatz falsch positiver und negativer Ergebnisse.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
